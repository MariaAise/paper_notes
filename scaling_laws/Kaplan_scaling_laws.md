Kaplan et al., 2020 â€” Scaling Laws for Neural Language Models
(OpenAI) 

Training budget is finite: how to allocate it efficienty  (optimization)

Used cross entropy loss to track the dependence between performance and data size, model size and compute time: signigicant changes

Insignificant: network architecture (deapth/width)  



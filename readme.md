# Topic: Scaling Laws & Compute-Optimal Training

- Core transformer scaling laws  
- Chinchilla compute-optimal training  
- Data quality scaling  
- Mixture-of-experts scaling behaviour  
- Long-context compute scaling  
- Inference-time scaling  

---

# Topic: Vision & Multimodal Transformers

- ViT + stable training strategies  
- Low-latency ViT architectures  
- Segment Anything (SAM 1 â†’ SAM3)  
- ImageBind & multi-sensory fusion  
- Video transformers (TimeSformer, ViViT, InternVideo)  

---

# Topic: Contrastive Learning & Multimodal Alignment

- CLIP architecture and dual encoders  
- Unified vs dual encoders  
- Contrastive + generative hybrid training  
- Cross-modal attention  
- Synthetic vs real caption data  
- Large-batch contrastive training  

---

# Topic: Preference Learning & Alignment (Post-RLHF)

- RLAIF (AI-generated preference data)  
- DPO, IPO, ORPO, KTO (RL-free preference optimisation)  
- Pairwise preference modelling  
- Synthetic reward modelling  
- Model-based evaluators  
- Behaviour shaping via curated preference datasets  

---

# Topic: Diffusion, Consistency, and Flow Models

- Latent diffusion for efficiency  
- Consistency Models (CMs)  
- Flow Matching (FM)  
- Rectified Flow models  
- 1-step distilled diffusion models  

---

# Topic: Domain Adaptation & Distribution Shift (Financial-Centric)

- Test-time adaptation (TTA / TENT)  
- Online shift adaptation  
- Dynamic feature/gate routing  
- Continual learning under non-stationarity  
- Synthetic-to-real adaptation for financial text  
- Financial time-series domain drift models  
- Uncertainty metrics for shift detection  

---

# Topic: Long-Context, Memory-Efficient, and SSM Models

- FlashAttention 2 / 3  
- State-Space Models (S4, Mamba)  
- RWKV  
- Hyena operators  
- Retentive networks  
- Mixture of Attention (MoA)  
- LongRoPE, YaRN  
- Streaming transformers  

---

# Topic: Mixture-of-Experts & Sparse Routing Systems

- Switch Transformer  
- GShard  
- DeepSeek-MoE architectures  
- Expert routing stability  
- Load balancing strategies  
- Sparse compute scaling laws  
- MoE inference-time routing optimisations  

---

# Topic: Efficient Training Frameworks & Distributed Systems

- FSDP  
- ZeRO-3  
- Fully Sharded Attention  
- Tensor parallelism  
- Pipeline parallelism  
- Flash decoders  
- Triton kernels  

---

# Topic: Optimization & Stability of Large Models

- Adam, AdamW  
- Lion optimizer  
- Sophia  
- Shampoo / Adafactor  
- RMSNorm and st
